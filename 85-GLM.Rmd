# Introduction to the GLM {#glm}

```{r echo = FALSE}
knitr::opts_chunk$set(fig.width = 4.5, fig.height = 4.5,
                      comment = NA, cache = TRUE)
options(rt.theme = "whitegrid")
options(rt.fit.theme = "whitegrid")
library(rtemis)
```

```{r, comment="", results="asis", echo=FALSE}
old.hooks <- fansi::set_knit_hooks(knitr::knit_hooks)
options(crayon.enabled = TRUE)
```

## Generalized Linear Model (GLM)

The [Generalized Linear Model](https://en.wikipedia.org/wiki/Generalized_linear_model) is one of the most popular and important models in statistics.  

It fits a model of the form:

$$y = β_0 + β_1 x_1 + β_2 x_2 + ... β_n x_n + ε$$
where: 

- $y$ is the dependent variable, i.e. outcome of interest
- $x_1$ to $x_n$ are the independent variables, a.k.a. covariates, a.k.a. predictors
- $β_0$ is the intercept
- $β_1$ to $β_n$ are the coefficients
- $ε$ is the error

In matrix notation:

$$y = Χβ + ε$$

Let's look at an example using the GLM for regression. We'll use the `diabetes` dataset from the `mlbench` package as an example.

```{r}
data(PimaIndiansDiabetes2, package = 'mlbench')
str(PimaIndiansDiabetes2)
```

We fit a model predicting glucose level from all other covariates:

```{r}
mod <- glm(glucose ~ ., family = "gaussian", data = PimaIndiansDiabetes2)
mod
class(mod)
```

The `glm()` function accepts a formula that defines the model.  
The formula `hp ~ .` means "regress hp on all other variables". The `family` argument defines we are performing regression and the `data` argument points to the data frame where the covariates used in the formula are found.

For a gaussian output, we can also use the `lm()` function. There are minor differences in the output created, but the model is the same:

```{r}
mod_lm <- lm(glucose ~ ., data = PimaIndiansDiabetes2)
mod_lm
class(mod_lm)
```

### Summary

Get summary of the model using `summary()`:

```{r}
summary(mod)
```

Note how R prints stars next to covariates whose p-values falls within certain limits, described right below the table of estimates.  

Also notice that categorical variables of n levels get n-1 separate coefficients; the first level is considered the baseline. Therefore, make sure to set up [factors](#factors) appropriately before modeling to ensure the correct level is used as baseline.

### Coefficients

```{r}
coefficients(mod)
# or
mod$coefficients
```

### Fitted values

```{r}
fitted(mod) |> head()
# or
mod$fitted.values |> head()
```

### Residuals

```{r}
residuals(mod) |> head()
# or
mod$residuals |> head()
```

### p-values

To extract the p-values of the intercept and each coefficient, we use `coef()` on `summary()`. The final (4th) column lists the p-values:

```{r}
coef(summary(mod))
```

### Plot linear fit

You use `lines()` to add a line on top of a scatterplot drawn with `plot()`.  
`lines()` accepts `x` and `y` vectors of coordinates:

```{r}
set.seed(2020)
x <- rnorm(500)
y <- .73 * x + .5 * rnorm(500)
xy.fit <- lm(y~x)$fitted
plot(x, y, pch = 16, col = "#18A3AC99")
lines(x, xy.fit, col = "#178CCB", lwd = 2)
```

In [**rtemis**](https://rtemis.lambdamd.org/), you can use argument `fit` to use any supported algorithm (see `modSelect()`) to estimate the fit:

```{r}
mplot3.xy(x, y, fit = "glm")
```

### Logistic Regression

For logistic regression, i.e. classification, you can use `glm()` with `family = binomial`

Using the same dataset, let's predict diabetes status:

```{r}
diabetes_mod <- glm(diabetes ~ ., PimaIndiansDiabetes2, 
                    family = "binomial")
diabetes_mod
```

```{r}
summary(diabetes_mod)
```

## Mass-univariate analysis

There are many cases where we have a large number of predictors and, along with any other number of tests or models, we may want to regress our outcome of interest on each covariate, one at a time.  

Let's create some synthetic data with 1000 cases and 100 covariates  
The outcome is generated using just 4 of those 100 covariates and has added noise.

```{r}
set.seed(2020)
n_col <- 100
n_row <- 1000
x <- as.data.frame(lapply(seq(n_col), function(i) rnorm(n_row)),
                   col.names = paste0("Feature_", seq(n_col)))
dim(x)
y <- .7 + x[, 10] + .3 * x[, 20] + 1.3 * x[, 30] + x[, 50] + rnorm(500)
```

Let's fit a linear model regressing y on each column of x using `lm`:

```{r}
mod.xy.massuni <- lapply(seq(x), function(i) lm(y ~ x[, i]))
length(mod.xy.massuni)
names(mod.xy.massuni) <- paste0("mod", seq(x))
```

To extract p-values for each model, we must find where exactly to look.  
Let's look into the first model:

```{r}
(ms1 <- summary(mod.xy.massuni$mod1))
ms1$coefficients
```

The p-values for each feature is stored in row 1, column 4 fo the coefficients matrix. Let's extract all of them:

```{r}
mod.xy.massuni.pvals <- sapply(mod.xy.massuni, function(i) summary(i)$coefficients[2, 4])
```

Let's see which variable are significant at the 0.05:

```{r}
which(mod.xy.massuni.pvals < .05)
```

...and which are significant at the 0.01 level:

```{r}
which(mod.xy.massuni.pvals < .01)
```

## Correction for multiple comparisons

We've performed a large number of tests and before reporting the results, we need to control for [multiple comparisons](https://en.wikipedia.org/wiki/Multiple_comparisons_problem).  
To do that, we use R's `p.adjust()` function. It adjusts a vector of p-values to account for multiple comparisons using one of multiple methods. The default, and recommended, is the [Holm method](https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method). It ensures that `FWER < α`, i.e. controls the [family-wise error rate](https://en.wikipedia.org/wiki/Family-wise_error_rate), a.k.a. the probability of making one or more false discoveries (Type I errors) 

```{r}
mod.xy.massuni.pvals.holm_adjusted <- p.adjust(mod.xy.massuni.pvals)
```

Now, let's see which features' p-values survive the magical .05 threshold:

```{r}
which(mod.xy.massuni.pvals.holm_adjusted < .05)
```

These are indeed the correct features (not surprisingly, still reassuringly).

## Resources {#glmresources}

- [Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models](https://regression.ucsf.edu/)
